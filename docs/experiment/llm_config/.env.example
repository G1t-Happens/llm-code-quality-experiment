# ===============================================================
# .env → docs/experiment/llm_config/.env
# ===============================================================

# API KEY(s)
# OPENAI_API_KEY=sk-

# Dein gewünschtes Modell
# OPENAI_MODEL=gpt-4.1
# OPENAI_MODEL=gpt-4o-mini                  # günstiger + schneller
# OPENAI_MODEL=o1-preview                   # wenn du Reasoning willst
# OPENAI_MODEL=gpt-4o-long-output-2025-08-27  # für extrem lange Outputs

# Optional: Azure, Groq, Together, etc.
# OPENAI_API_BASE=https://api.groq.com/openai/v1

# --- Steuerbare Parameter ---
# TEMPERATURE=0.0
# MAX_TOKENS=32768                    # für normale Modelle (wird bei Long-Output automatisch auf 64k erhöht)
# O1_MAX_COMPLETION_TOKENS=32768      # für o1/o3-Modelle
# REQUEST_TIMEOUT=600
# TOP_P=0.90

# Debug-Modus (zeigt mehr Infos)
# DEBUG=1